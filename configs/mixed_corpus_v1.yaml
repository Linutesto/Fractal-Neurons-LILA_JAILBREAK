# Mixed corpus model training configuration
seed: 1337

data:
  source: textdir
  text_root: data/mixed_corpus
  text_globs:
    - "mixed_data.jsonl"
  exclude_globs:
    - "**/.git/**"
    - "**/.cache/**"
    - "**/__pycache__/**"
  seq_len: 1024
  stride: 1024
  mask_rate: 0.15
  text_use_cache: true
  text_refresh_cache: false
  text_shuffle_buffer: 4096
  text_tokenize_in_workers: true
  text_reshuffle_each_epoch: true
  text_seed: 1337

model:
  dim: 512
  depth: 16
  fanout: 2
  use_fp16: true
  droppath_rate: 0.1
  branch_dropout: 0.1
  interconnect: true
  interconnect_heads: 4
  interconnect_dropout: 0.05
  num_experts: 4
  expert_hidden: 384
  expert_top_k: 1
  capacity_factor: 1.0
  moe_aux_lambda: 0.02

train:
  run_name: mixed_corpus_v1
  out_dir: runs
  device: cuda
  batch_size: 8
  steps: 100000
  lr: 1.0e-4
  weight_decay: 0.02
  grad_accum: 16
  seed: 1337
  deterministic: false
  resume: false
  num_workers: 16
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  torch_compile: false
  tf32: true
  log_every: 50
  ckpt_every: 1000
  warmup_steps: 1000
  lr_min: 1.0e-5
  cosine_cycle: 0
  grad_clip: 1.0
  use_ema: true
  ema_decay: 0.9995
  ga_enable: true
  ga_every: 500
  ga_population: 8
  ga_sigma: 0.05
  prefetch_gpu: true
  prefetch_gpu_batches: 2
  adam_fused: true
  compile_mode: default

moe:
  aux_coeff: 0.02

tokenizer:
  type: hf
  name_or_path: runs/fsi_en_v1/tokenizer
  truncation_side: right
  pad_to_multiple_of: 128
